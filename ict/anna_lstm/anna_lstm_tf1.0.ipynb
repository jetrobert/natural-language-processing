{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《安娜卡列尼娜》新编——利用TensorFlow构建LSTM模型\n",
    "\n",
    "最近看完了LSTM的一些外文资料，主要参考了Colah的blog以及Andrej Karpathy blog的一些关于RNN的材料，准备动手去实现一个LSTM模型。代码的基础框架来自于Udacity上深度学习纳米学位的课程（付费课程）的一个demo，我刚开始看代码的时候真的是一头雾水，很多东西没有理解，后来反复查阅资料，并我重新对代码进行了学习和修改，对步骤进行了进一步的剖析，下面将一步步用TensorFlow来构建LSTM模型进行文本学习并试图去生成新的文本。\n",
    "\n",
    "关于RNN与LSTM模型本文不做介绍，详情去查阅资料过着去看上面的blog链接，讲的很清楚啦。这篇文章主要是偏向实战，来自己动手构建LSTM模型。\n",
    "\n",
    "数据集来自于外文版《安娜卡列妮娜》书籍的文本文档（本文后面会提供整个project的git链接）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 61,  0, 66,  3, 42, 11, 48, 73, 16, 16, 16, 68,  0, 66, 66, 74,\n",
       "       48, 39,  0, 72, 82,  5, 82, 42, 34, 48,  0, 11, 42, 48,  0,  5,  5,\n",
       "       48,  0,  5, 82,  6, 42, 12, 48, 42, 55, 42, 11, 74, 48, 18, 27, 61,\n",
       "        0, 66, 66, 74, 48, 39,  0, 72, 82,  5, 74, 48, 82, 34, 48, 18, 27,\n",
       "       61,  0, 66, 66, 74, 48, 82, 27, 48, 82,  3, 34, 48, 64, 75, 27, 16,\n",
       "       75,  0, 74, 47, 16, 16, 80, 55, 42, 11, 74,  3, 61, 82, 27])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 分割mini-batch\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "完成了前面的数据预处理操作，接下来就是要划分我们的数据集，在这里我们使用mini-batch来进行模型训练，那么我们要如何划分数据集呢？在进行mini-batch划分之前，我们先来了解几个概念。\n",
    "\n",
    "假如我们目前手里有一个序列1-12，我们接下来以这个序列为例来说明划分mini-batch中的几个概念。首先我们回顾一下，在DNN和CNN中，我们都会将数据分batch输入给神经网络，加入我们有100个样本，如果设置我们的batch_size=10，那么意味着每次我们都会向神经网络输入10个样本进行训练调整参数。同样的，在LSTM中，batch_size意味着每次向网络输入多少个样本，在上图中，当我们设置batch_size=2时，我们会将整个序列划分为6个batch，每个batch中有两个数字。\n",
    "\n",
    "然而由于RNN中存在着“记忆”，也就是循环。事实上一个循环神经网络能够被看做是多个相同神经网络的叠加，在这个系统中，每一个网络都会传递信息给下一个。上面的图中，我们可以看到整个RNN网络由三个相同的神经网络单元叠加起来的序列。那么在这里就有了第二个概念sequence_length（也叫steps），中文叫序列长度。上图中序列长度是3，可以看到将三个字符作为了一个序列。\n",
    "\n",
    "有了上面两个概念，我们来规范一下后面的定义。我们定义一个batch中的序列个数为N（batch_size），定义单个序列长度为M（也就是我们的steps）。那么实际上我们每个batch是一个N x M的数组。在这里我们重新定义batch_size为一个N x M的数组，而不是batch中序列的个数。在上图中，当我们设置N=2， M=3时，我们可以得到每个batch的大小为2 x 3 = 6个字符，整个序列可以被分割成12 / 6 = 2个batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch分割\n",
    "    \n",
    "    arr: 待分割的数组\n",
    "    n_seqs: 一个batch中序列个数\n",
    "    n_steps: 单个序列包含的字符数\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(arr) / batch_size)\n",
    "    # 这里我们仅保留完整的batch，对于不能整出的部分进行舍弃\n",
    "    arr = arr[:batch_size * n_batches]\n",
    "    \n",
    "    # 重塑\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # inputs\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码定义了一个generator，调用函数会返回一个generator对象，我们可以获取一个batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[38 61  0 66  3 42 11 48 73 16]\n",
      " [48  0 72 48 27 64  3 48 67 64]\n",
      " [55 82 27 47 16 16 20 45 42 34]\n",
      " [27 48  7 18 11 82 27 67 48 61]\n",
      " [48 82  3 48 82 34 62 48 34 82]\n",
      " [48 54  3 48 75  0 34 16 64 27]\n",
      " [61 42 27 48 44 64 72 42 48 39]\n",
      " [12 48 33 18  3 48 27 64 75 48]\n",
      " [ 3 48 82 34 27  2  3 47 48 41]\n",
      " [48 34  0 82  7 48  3 64 48 61]]\n",
      "\n",
      "y\n",
      " [[61  0 66  3 42 11 48 73 16 16]\n",
      " [ 0 72 48 27 64  3 48 67 64 82]\n",
      " [82 27 47 16 16 20 45 42 34 62]\n",
      " [48  7 18 11 82 27 67 48 61 82]\n",
      " [82  3 48 82 34 62 48 34 82 11]\n",
      " [54  3 48 75  0 34 16 64 27  5]\n",
      " [42 27 48 44 64 72 42 48 39 64]\n",
      " [48 33 18  3 48 27 64 75 48 34]\n",
      " [48 82 34 27  2  3 47 48 41 61]\n",
      " [34  0 82  7 48  3 64 48 61 42]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 模型构建\n",
    "模型构建部分主要包括了输入层，LSTM层，输出层，loss，optimizer等部分的构建，我们将一块一块来进行实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs, num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    \n",
    "    num_seqs: 每个batch中的序列个数\n",
    "    num_steps: 每个序列包含的字符数\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, shape=(num_seqs, num_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(num_seqs, num_steps), name='targets')\n",
    "    \n",
    "    # 加入keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 LSTM层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' \n",
    "    构建lstm层\n",
    "        \n",
    "    keep_prob\n",
    "    lstm_size: lstm隐层中结点数目\n",
    "    num_layers: lstm的隐层数目\n",
    "    batch_size: batch_size\n",
    "\n",
    "    '''\n",
    "    def lstm_cell():\n",
    "        cell = tf.contrib.rnn.NASCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "        return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    # 构建一个基本lstm单元\n",
    "    #lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    #lstm = tf.contrib.rnn.NASCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "    \n",
    "    # 添加dropout\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # 堆叠\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple = True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' \n",
    "    构造输出层\n",
    "        \n",
    "    lstm_output: lstm层的输出结果\n",
    "    in_size: lstm输出层重塑后的size\n",
    "    out_size: softmax层的size\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # 将lstm的输出按照列concate，例如[[1,2,3],[7,8,9]],\n",
    "    # tf.concat的结果是[1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, axis=1) # tf.concat(concat_dim, values)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # 将lstm层与softmax层全连接\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # 计算logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmax层返回概率分布\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 训练误差计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    根据logits和targets计算损失\n",
    "    \n",
    "    logits: 全连接层的输出结果（不经过softmax）\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot编码\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Optimizer\n",
    "我们知道RNN会遇到梯度爆炸（gradients exploding）和梯度弥散（gradients disappearing)的问题。LSTM解决了梯度弥散的问题，但是gradient仍然可能会爆炸，因此我们采用gradient clippling的方式来防止梯度爆炸。即通过设置一个阈值，当gradients超过这个阈值时，就将它重置为阈值大小，这就保证了梯度不会变得很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    构造Optimizer\n",
    "   \n",
    "    loss: 损失\n",
    "    learning_rate: 学习率\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 使用clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 模型组合\n",
    "使用tf.nn.dynamic_run来运行RNN序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # 如果sampling是True，则采用SGD\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 输入层\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # LSTM层\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        # 对输入进行one-hot编码\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # 运行RNN\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 预测结果\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss 和 optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置\n",
    "在模型训练之前，我们首先初始化一些参数，我们的参数主要有：\n",
    "\n",
    "- num_seqs: 单个batch中序列的个数\n",
    "- num_steps: 单个序列中字符数目\n",
    "- lstm_size: 隐层结点个数\n",
    "- num_layers: LSTM层个数\n",
    "- learning_rate: 学习率\n",
    "- keep_prob: dropout层中保留结点比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮数: 1/20...  训练步数: 100...  训练误差: 3.1381...  15.6218 sec/batch\n",
      "轮数: 2/20...  训练步数: 200...  训练误差: 3.0576...  15.3691 sec/batch\n",
      "轮数: 2/20...  训练步数: 300...  训练误差: 2.2834...  15.2008 sec/batch\n",
      "轮数: 3/20...  训练步数: 400...  训练误差: 2.0106...  15.0065 sec/batch\n",
      "轮数: 3/20...  训练步数: 500...  训练误差: 1.8685...  15.1701 sec/batch\n",
      "轮数: 4/20...  训练步数: 600...  训练误差: 1.7585...  15.2357 sec/batch\n",
      "轮数: 4/20...  训练步数: 700...  训练误差: 1.7203...  14.8942 sec/batch\n",
      "轮数: 5/20...  训练步数: 800...  训练误差: 1.6712...  15.1200 sec/batch\n",
      "轮数: 5/20...  训练步数: 900...  训练误差: 1.6255...  14.6830 sec/batch\n",
      "轮数: 6/20...  训练步数: 1000...  训练误差: 1.5692...  14.8890 sec/batch\n",
      "轮数: 6/20...  训练步数: 1100...  训练误差: 1.5646...  14.6885 sec/batch\n",
      "轮数: 7/20...  训练步数: 1200...  训练误差: 1.5088...  15.0596 sec/batch\n",
      "轮数: 7/20...  训练步数: 1300...  训练误差: 1.4944...  14.3597 sec/batch\n",
      "轮数: 8/20...  训练步数: 1400...  训练误差: 1.5092...  15.2534 sec/batch\n",
      "轮数: 8/20...  训练步数: 1500...  训练误差: 1.4474...  14.8528 sec/batch\n",
      "轮数: 9/20...  训练步数: 1600...  训练误差: 1.4111...  15.1138 sec/batch\n",
      "轮数: 9/20...  训练步数: 1700...  训练误差: 1.3924...  14.5741 sec/batch\n",
      "轮数: 10/20...  训练步数: 1800...  训练误差: 1.4430...  14.6832 sec/batch\n",
      "轮数: 10/20...  训练步数: 1900...  训练误差: 1.4049...  14.5824 sec/batch\n",
      "轮数: 11/20...  训练步数: 2000...  训练误差: 1.4167...  15.8140 sec/batch\n",
      "轮数: 11/20...  训练步数: 2100...  训练误差: 1.3684...  14.4397 sec/batch\n",
      "轮数: 12/20...  训练步数: 2200...  训练误差: 1.3867...  14.4359 sec/batch\n",
      "轮数: 12/20...  训练步数: 2300...  训练误差: 1.3215...  16.7657 sec/batch\n",
      "轮数: 13/20...  训练步数: 2400...  训练误差: 1.3656...  17.6767 sec/batch\n",
      "轮数: 13/20...  训练步数: 2500...  训练误差: 1.3501...  15.0877 sec/batch\n",
      "轮数: 14/20...  训练步数: 2600...  训练误差: 1.2988...  15.0809 sec/batch\n",
      "轮数: 14/20...  训练步数: 2700...  训练误差: 1.2856...  14.5554 sec/batch\n",
      "轮数: 15/20...  训练步数: 2800...  训练误差: 1.3395...  15.2094 sec/batch\n",
      "轮数: 15/20...  训练步数: 2900...  训练误差: 1.3164...  15.3312 sec/batch\n",
      "轮数: 16/20...  训练步数: 3000...  训练误差: 1.3328...  15.4234 sec/batch\n",
      "轮数: 16/20...  训练步数: 3100...  训练误差: 1.2648...  15.6458 sec/batch\n",
      "轮数: 17/20...  训练步数: 3200...  训练误差: 1.2759...  14.2783 sec/batch\n",
      "轮数: 17/20...  训练步数: 3300...  训练误差: 1.2709...  13.7485 sec/batch\n",
      "轮数: 18/20...  训练步数: 3400...  训练误差: 1.3002...  14.8954 sec/batch\n",
      "轮数: 18/20...  训练步数: 3500...  训练误差: 1.2913...  14.5881 sec/batch\n",
      "轮数: 19/20...  训练步数: 3600...  训练误差: 1.2754...  14.2527 sec/batch\n",
      "轮数: 19/20...  训练步数: 3700...  训练误差: 1.2702...  14.4383 sec/batch\n",
      "轮数: 20/20...  训练步数: 3800...  训练误差: 1.2240...  15.1219 sec/batch\n",
      "轮数: 20/20...  训练步数: 3900...  训练误差: 1.2773...  16.4926 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# 每n轮进行一次变量保存\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            # control the print lines\n",
    "            if counter % 100 == 0:\n",
    "                print('轮数: {}/{}... '.format(e+1, epochs),\n",
    "                      '训练步数: {}... '.format(counter),\n",
    "                      '训练误差: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3960_l512.ckpt\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 文本生成\n",
    "现在我们可以基于我们的训练参数进行文本的生成。当我们输入一个字符时，LSTM会预测下一个字符，我们再将新的字符进行输入，这样能不断的循环下去生成本文。\n",
    "\n",
    "为了减少噪音，每次的预测值我会选择最可能的前5个进行随机选择，比如输入h，预测结果概率最大的前五个为[o,e,i,u,b]，我们将随机从这五个中挑选一个作为新的字符，让过程加入随机因素会减少一些噪音的生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    \"\"\"\n",
    "    从预测结果中选取前top_n个最可能的字符\n",
    "    \n",
    "    preds: 预测结果\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    # 将除了top_n个预测值的位置都置为0\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    # 归一化概率\n",
    "    p = p / np.sum(p)\n",
    "    # 随机选取一个字符\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    \"\"\"\n",
    "    生成新文本\n",
    "    \n",
    "    checkpoint: 某一轮迭代的参数文件\n",
    "    n_sample: 新闻本的字符长度\n",
    "    lstm_size: 隐层结点数\n",
    "    vocab_size\n",
    "    prime: 起始文本\n",
    "    \"\"\"\n",
    "    # 将输入的单词转换为单个字符组成的list\n",
    "    samples = [c for c in prime]\n",
    "    # sampling=True意味着batch的size=1 x 1\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 加载模型参数，恢复训练\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            # 输入单个字符\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        # 添加字符到samples中\n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        # 不断生成字符，直到达到指定数目\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i3960_l512.ckpt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3960_l512.ckpt\n",
      "Therother.\n",
      "\n",
      "\"Yes, but you see that then some true and to come to them. There was a self-completely\n",
      "sheal, that interests and sense that they was now something to she say to him.\n",
      "\n",
      "And the steps had been contrally important that he had been tried.\n",
      "\n",
      "Sergey Ivanovitch saw it in the sense and a prince. He would\n",
      "not be the same tea to talk to her has been\n",
      "so abstrectly.\n",
      "\n",
      "\"Why do you think of you,\" he said as as he she had not\n",
      "saying.\n",
      "\n",
      "\"I shall need becoming in the same time. Taken, it was the prunt in\n",
      "his brother to say to all the party to suppose.\n",
      "\n",
      "\"Woman in the creach, that's the past, that is the secret of the plais in the\n",
      "country. There's not to see it, I always been a crocker work is\n",
      "seeing these position and all and make all an intellect,\n",
      "without which, that was talking to him that would\n",
      "say wearing her; and stopped her husband's\n",
      "face. He could not be a little consequence with which they had\n",
      "been a shirt for her. Though they were alone to her. He had\n",
      "never seen them of his worth, had been an old members and tendelly and all something what was at\n",
      "her son. He was there was nothing about it, and her face at the big reconciled\n",
      "and something flown and\n",
      "so to anything with her\n",
      "head and sat down and showing and her face,\n",
      "he went into the subject. She had not heard this,\n",
      "the prince. And they said when he had not been, at once with his brother's\n",
      "frightening to him when he heard in what she had not to call to say\n",
      "in her and the cause of that secantal side-to the peasant, who had no new heart\n",
      "to her.\n",
      "\n",
      "\"What has should be to try his wife. He's an impressions, the\n",
      "desarers and his heart to speak about it. It's so marvelous at once.\n",
      "And that to bring her to hear all of the base, and\n",
      "I sat down, that's this sort of this carrying and too,\" thought Veslovsky.\n",
      "\n",
      "\"I see the minute. And there's so mother...\"\n",
      "\n",
      "\"Yes, I am to go in all signed of her horror that they has a previous and happy in the same as\n",
      "impotitiously so it with a subject that if it was the prayer with them\n",
      "in a service in the\n"
     ]
    }
   ],
   "source": [
    "# 选用最终的训练参数作为输入进行文本生成\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"The\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "Faratttto  oto  ate eeneen atthen  hta   teoe eee  taethea  etanennnt h o hoe e ee ta ha hanaent    oeat aa ha t et eo an eeee t eeenn ata e h ooet  ee att   ha  aennae  et   te tt h teethonth ho ho hteeeteas a ene  h een  ae  e o tatan a te eaa a eteaestea ten  te h  ae t tattaentt hoa    toonee  e   oo ae  a  tteete  h  a   ete aat  a t  te  ean e ht t haee   hteon  ao  t   t a ha a han a ttteaet e  etoe t  e ta   e e een eaat h tta a te    o a     t  a h ot te ae ae e a a ane  hoot he ae h  tthte hoeee t e  h etees  t h  a hoota haeen  eeee ha e te  a ae anna  e oetot tho  tetthoaa ene  a he  e aea tea  eet ha tteethaeee a ae  a e hae e taet  he  heet  ho h   ot  at e eee  te  h ettt ht e ttht taa ha htat ateta e ett e h  a e a h o e  eon eent t  a aae ttee  han etae hae  e ane ana h teetta tann aee a e t ae eeas t  te een ane tetaaa ea t hae eee haeeenn e ae to to h aea eeeesta  teaa   a h  a  eaee eaanettet eo e e ee hte t  teeanst eo aa a ae  aa e ha  a ht    a    aona  tetathae t ho\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1000_l512.ckpt\n",
      "Farery, and to his forget the sempons. She heanded her heard they terraced to her, and he had anderstand.\n",
      "\n",
      "\"That!\" said Vronsky thought of a little always with his\n",
      "even and his same timing what at the capess with any starting her figure overstily and her houre that in this heart him and the served and all a lood, he distined him\n",
      "to a paintation of at him.\n",
      "\n",
      "\"Which would be and asked they words of sen that\n",
      "they were not them. It\n",
      "had a promistalled the sain, and here to him, her highle of this he had\n",
      "been\n",
      "a musim,\n",
      "and had\n",
      "not to held he\n",
      "made\n",
      "her astentations. And trying her,\n",
      "as his stood that he shighted a distracted her harres of the\n",
      "same\n",
      "the sacks inte still to his had and the strugh of her fring a lowes.\"\n",
      "\n",
      "\"What with me and though it had never been an expression\n",
      "of the contingat and, the seemed, treating the concerte to be to be as that wife was himself, that, all the sent to her for her time the\n",
      "consirity of at his cares, and was at at help, that he heard he would not to be a standiously \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l512.ckpt\n",
      "Farouse of when he could be\n",
      "the chief cold and anyway.\n",
      "\n",
      "\"Why? What as sha plush, as\n",
      "she would never to get him.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 21\n",
      "\n",
      "\n",
      "\"And I shall not a set, if you say it. Why, you want on your high sort in a man, when she was a stit wanted,\" answered the\n",
      "carriages transing on them, and a least as is all on\n",
      "his hands that he could be success to the pattle support, and\n",
      "at how she had\n",
      "been the still.\n",
      "\n",
      "\"Yes, I have now it's a seemed the peace. If\n",
      "you know a little, and I do not\n",
      "the strange of time. It's to took the mistill men about the cross of the sourd of this man, any the cass works.\n",
      "\n",
      "Tree, and had not\n",
      "been and stating at the\n",
      "son was all that he had been drove on tearity and\n",
      "set in a passed thinks and take that he had supposed all of the more of\n",
      "interpretition of the corner when the prevorced thought. \"I have a chair. I had saw her to\n",
      "the\n",
      "point that he saw someone to see you. Watch of me, and that hands. It's so many about a sected.\"\n",
      "\n",
      "The part of the care. Time to\n",
      "the painting in the completel\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i2000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
